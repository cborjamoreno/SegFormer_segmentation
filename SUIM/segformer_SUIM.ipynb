{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, TFSegformerForSemanticSegmentation\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "# import albumentations as aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 640\n",
    "HEIGHT = 480\n",
    "dataset_path = 'SUIM_sam_100'\n",
    "output_path = 'test_predictions_sam_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "\n",
    "        sub_path = \"train\" if self.train else \"test\"\n",
    "        self.img_dir = os.path.join(self.root_dir, sub_path, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, sub_path, \"masks\")\n",
    "\n",
    "        print(self.img_dir)\n",
    "        print(self.ann_dir)\n",
    "        \n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct paths to the image and annotation\n",
    "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        annotation_path = os.path.join(self.ann_dir, self.annotations[idx])\n",
    "\n",
    "        # Read the image and annotation using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        segmentation_map = cv2.imread(annotation_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Dictionary mapping original IDs to new class IDs\n",
    "        num2id = {0: 26, 1: 0, 2: 29, 3: 150, 4: 179, 5: 76, 6: 105, 7: 226, 8: 255}\n",
    "        id_values = np.array(list(num2id.values()))\n",
    "        id_keys = np.array(list(num2id.keys()))\n",
    "\n",
    "        # Function to find the key of the closest value in num2id values\n",
    "        def find_closest_key(value, id_values, id_keys):\n",
    "            index = np.abs(id_values - value).argmin()\n",
    "            return id_keys[index]\n",
    "\n",
    "        # Create a mapping for each unique value in segmentation_map to the key of its closest value in num2id values\n",
    "        unique_values = np.unique(segmentation_map)\n",
    "        closest_key_mapping = {val: find_closest_key(val, id_values, id_keys) for val in unique_values}\n",
    "\n",
    "        # Transform segmentation_map to have the keys corresponding to the closest num2id values\n",
    "        converted_map = np.copy(segmentation_map)\n",
    "        for original, closest_key in closest_key_mapping.items():\n",
    "            converted_map[segmentation_map == original] = closest_key\n",
    "\n",
    "        # Update segmentation_map with the converted_map\n",
    "        segmentation_map = converted_map\n",
    "        \n",
    "        # Convert the OpenCV image to a PIL Image for the transformation\n",
    "        image = Image.fromarray(image)\n",
    "        segmentation_map = Image.fromarray(segmentation_map)\n",
    "\n",
    "        # Resize the image and segmentation map to a fixed size, e.g., 640x480\n",
    "        image = TF.resize(image, (HEIGHT, WIDTH))\n",
    "        segmentation_map = TF.resize(segmentation_map, (HEIGHT, WIDTH), interpolation=TF.InterpolationMode.NEAREST)\n",
    "\n",
    "        # Apply the transformations if any\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            # Convert the PIL Image to a tensor (this also permutes dimensions to C x H x W)\n",
    "            image = TF.to_tensor(image)\n",
    "\n",
    "        # Convert the PIL Image back to a NumPy array if your processing pipeline requires it\n",
    "        segmentation_map = np.array(segmentation_map)\n",
    "\n",
    "        # Convert the segmentation map to a tensor\n",
    "        segmentation_map = torch.tensor(segmentation_map, dtype=torch.long)\n",
    "\n",
    "\n",
    "        # Prepare the return dictionary\n",
    "        return_dict = {'pixel_values': image, 'labels': segmentation_map}\n",
    "\n",
    "        # Include the filename in the return dictionary if not in training mode\n",
    "        # Inside your method, after checking if not in training mode\n",
    "        if not self.train:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "            return_dict['filename'] = self.images[idx]\n",
    "            return_dict['dim'] = (width, height)\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(0.5),\n",
    "    # transforms.RandomVerticalFlip(0.05),\n",
    "    # transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(root_dir=dataset_path, feature_extractor=feature_extractor, transforms=transform)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=dataset_path, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = train_dataset[1]\n",
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs[\"labels\"].squeeze().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = encoded_inputs[\"labels\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.8)  # 80% for training\n",
    "validation_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Splitting the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(validation_dataset, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"backgrond\", 1: \"BW\", 2: \"HD\", 3: \"PF\", 4: \"WR\", 5: \"RO\", 6: \"RI\", 7: \"FV\", 8: \"SR\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"nvidia/mit-b0\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(1, num_epochs+1):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        \n",
    "        #print unique labels\n",
    "        \n",
    "\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pixel_values = pixel_values.permute(0, 3, 1, 2)\n",
    "\n",
    "        # print(pixel_values.shape)\n",
    "        # print(pixel_values.dtype)\n",
    "        # print(labels.shape)\n",
    "        # print(labels.dtype)\n",
    "\n",
    "        # check that all labels values are within the range of the number of classes\n",
    "        assert torch.all(labels < num_labels), \"A label value is out of the range of the number of classes.\"\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "        # Check if the mask is not empty\n",
    "        # print(labels)\n",
    "        if mask.any():\n",
    "            pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "            true_labels = labels[mask].detach().cpu().numpy()\n",
    "            accuracy = accuracy_score(pred_labels, true_labels)\n",
    "        else:\n",
    "            print(\"Warning: No valid labels found for accuracy calculation.\")\n",
    "            accuracy = 0\n",
    "        loss = outputs.loss\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(valid_dataloader):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "                mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "                pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "                true_labels = labels[mask].detach().cpu().numpy()\n",
    "                accuracy = accuracy_score(pred_labels, true_labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_accuracies.append(accuracy)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
    "         Train Loss: {sum(losses)/len(losses)}\\\n",
    "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
    "         Val Loss: {sum(val_losses)/len(val_losses)}\")\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), dataset_path+\".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT IN TEST IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"backgrond\", 1: \"BW\", 2: \"HD\", 3: \"PF\", 4: \"WR\", 5: \"RO\", 6: \"RI\", 7: \"FV\", 8: \"SR\"}\n",
    "palette = [[64, 0, 64], [0, 0, 0], [0, 0, 255], [0, 255, 0], [0, 255, 255], [255, 0, 0], [255, 0, 255], [255, 255, 0], [255, 255, 255]]\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict in test images\n",
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "checkpoint = torch.load(dataset_path+\".pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def rescale_image(image, new_shape):\n",
    "    return cv2.resize(image, (new_shape[1], new_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def get_predictions(predictions_numpy, batch):\n",
    "    # Get the image sizes\n",
    "    original_shape = batch['pixel_values'].shape[-2:]\n",
    "    \n",
    "    # Rescale the predictions to the original image size\n",
    "    rescaled_predictions = [rescale_image(prediction, original_shape) for prediction in predictions_numpy]\n",
    "    \n",
    "    # Convert list of rescaled predictions to a NumPy array\n",
    "    rescaled_predictions = np.array(rescaled_predictions)\n",
    "    \n",
    "    # Get the original images and labels\n",
    "    images = batch['pixel_values'].cpu().numpy()\n",
    "    images = images.transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    return images, labels, rescaled_predictions\n",
    "\n",
    "# Plot the images, labels, and predictions\n",
    "def plot_predictions(images, labels, predictions):\n",
    "    fig, axs = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
    "    for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
    "        axs[i, 0].imshow(image)\n",
    "        axs[i, 0].set_title(\"Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(label)\n",
    "        axs[i, 1].set_title(\"Label\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        axs[i, 2].imshow(prediction)\n",
    "        axs[i, 2].set_title(\"Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def label_to_rgb(label, palette):\n",
    "    # Create an empty image with 3 channels for RGB\n",
    "    rgb_image = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Map each label to its corresponding color\n",
    "    for i, color in enumerate(palette):\n",
    "        mask = (label == i)\n",
    "        rgb_image[mask] = color\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def convert_predictions_and_labels_to_rgb(labels, predictions, palette):\n",
    "    labels_rgb = np.array([label_to_rgb(label, palette) for label in labels])\n",
    "    predictions_rgb = np.array([label_to_rgb(prediction, palette) for prediction in predictions])\n",
    "    return labels_rgb, predictions_rgb\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['pixel_values'].to(device)\n",
    "        filenames = batch['filename']\n",
    "        dims = batch['dim']\n",
    "\n",
    "        widths, heights = dims\n",
    "        widths_list = widths.tolist()\n",
    "        heights_list = heights.tolist()\n",
    "        dims_tuples = list(zip(widths_list, heights_list))\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Move predictions to CPU and convert to numpy for further processing if necessary\n",
    "        predictions_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "        # Process and plot predictions for the current batch\n",
    "        images, labels, rescaled_predictions = get_predictions(predictions_numpy, batch)\n",
    "\n",
    "        # Labels and predictions to RGB with palette\n",
    "        labels_rgb, predictions_rgb = convert_predictions_and_labels_to_rgb(labels, rescaled_predictions, palette)\n",
    "\n",
    "        for filename, prediction_rgb, dim in zip(filenames, predictions_rgb, dims_tuples):\n",
    "            all_predictions.append((filename, prediction_rgb, dim))\n",
    "\n",
    "# Reshape all predicitons to the original image size\n",
    "\n",
    "output_dir = output_path\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename, prediction, dim in all_predictions:  # Adjusted to unpack filenames with predictions\n",
    "    # Reshape prediction to the original image size\n",
    "    prediction = cv2.resize(prediction, dim, interpolation=cv2.INTER_NEAREST)\n",
    "    output_path = os.path.join(output_dir, os.path.splitext(filename)[0] + '.png')  # Use original filename\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(prediction, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
