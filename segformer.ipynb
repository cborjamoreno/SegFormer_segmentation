{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, TFSegformerForSemanticSegmentation\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "# import albumentations as aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 640\n",
    "HEIGHT = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "\n",
    "        sub_path = \"train\" if self.train else \"test\"\n",
    "        self.img_dir = os.path.join(self.root_dir, sub_path, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, sub_path, \"masks\")\n",
    "\n",
    "        print(self.img_dir)\n",
    "        print(self.ann_dir)\n",
    "        \n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct paths to the image and annotation\n",
    "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        annotation_path = os.path.join(self.ann_dir, self.annotations[idx])\n",
    "\n",
    "        # Read the image and annotation using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        segmentation_map = cv2.imread(annotation_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Convert segmentation map IDs to your dataset's specific class IDs\n",
    "        num2id = {0: 0, 1: 29, 2: 150, 3: 179, 4: 76, 5: 105, 6: 226, 7: 255}\n",
    "        converted_map = np.zeros_like(segmentation_map)\n",
    "        for num, id_ in num2id.items():\n",
    "            converted_map[segmentation_map == id_] = num\n",
    "        segmentation_map = converted_map\n",
    "        \n",
    "\n",
    "        # Convert the OpenCV image to a PIL Image for the transformation\n",
    "        image = Image.fromarray(image)\n",
    "        segmentation_map = Image.fromarray(segmentation_map)\n",
    "\n",
    "        # Resize the image and segmentation map to a fixed size, e.g., 640x480\n",
    "        image = TF.resize(image, (HEIGHT, WIDTH))\n",
    "        segmentation_map = TF.resize(segmentation_map, (HEIGHT, WIDTH), interpolation=TF.InterpolationMode.NEAREST)\n",
    "\n",
    "        # Apply the transformations if any\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            # Convert the PIL Image to a tensor (this also permutes dimensions to C x H x W)\n",
    "            image = TF.to_tensor(image)\n",
    "\n",
    "        # Convert the PIL Image back to a NumPy array if your processing pipeline requires it\n",
    "        segmentation_map = np.array(segmentation_map)\n",
    "\n",
    "        # Convert the segmentation map to a tensor\n",
    "        segmentation_map = torch.tensor(segmentation_map, dtype=torch.long)\n",
    "\n",
    "\n",
    "        # Prepare the return dictionary\n",
    "        return_dict = {'pixel_values': image, 'labels': segmentation_map}\n",
    "\n",
    "        # Include the filename in the return dictionary if not in training mode\n",
    "        # Inside your method, after checking if not in training mode\n",
    "        if not self.train:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "            return_dict['filename'] = self.images[idx]\n",
    "            return_dict['dim'] = (width, height)\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.05),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUIM/train/images\n",
      "SUIM/train/masks\n",
      "SUIM/test/images\n",
      "SUIM/test/masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbm/miniconda3/envs/mmlab/lib/python3.7/site-packages/transformers/models/segformer/feature_extraction_segformer.py:31: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'SUIM'\n",
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1508\n",
      "Number of validation examples: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 640])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = train_dataset[0]\n",
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 640])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [7, 7, 7,  ..., 3, 3, 3],\n",
       "        [7, 7, 7,  ..., 3, 3, 3],\n",
       "        [7, 7, 7,  ..., 3, 3, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].squeeze().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = encoded_inputs[\"labels\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7713683820d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.8)  # 80% for training\n",
    "validation_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Splitting the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(validation_dataset, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 480, 640])\n",
      "labels torch.Size([4, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 480, 640])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"BW\", 1: \"HD\", 2: \"PF\", 3: \"WR\", 4: \"RO\", 5: \"RI\", 6: \"FV\", 7: \"SR\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"nvidia/mit-b0\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbm/miniconda3/envs/mmlab/lib/python3.7/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35781d9e449043aeb94b378f22e60ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.4000026117013314         Train Loss: 1.4526835932636892         Val Pixel-wise accuracy: 0.4760756915053216         Val Loss: 1.4138463131691281\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80011daf9e44dc6be618fab2e98c928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.4292947209464326         Train Loss: 1.3797450213637572         Val Pixel-wise accuracy: 0.49368064295049596         Val Loss: 1.326500065232578\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f266f25455184ee4a87fdf7cfb65744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.436096300180654         Train Loss: 1.342796289368181         Val Pixel-wise accuracy: 0.3198694627975646         Val Loss: 1.390843592976269\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36f806e8b44d0e8ae1f73428ed4265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.4435376986802465         Train Loss: 1.3282065602722546         Val Pixel-wise accuracy: 0.42799804424414273         Val Loss: 1.2766675055027008\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90029272beb3480885c11c8c94a4e354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.4489081032221531         Train Loss: 1.3024005116216395         Val Pixel-wise accuracy: 0.502820806763961         Val Loss: 1.1977484343867553\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046473531bbc4d8f95fb03612e1ef69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6998/4114138074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmlab/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mmlab/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs+1):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pixel_values = pixel_values.permute(0, 3, 1, 2)\n",
    "\n",
    "        # print(pixel_values.shape)\n",
    "        # print(pixel_values.dtype)\n",
    "        # print(labels.shape)\n",
    "        # print(labels.dtype)\n",
    "\n",
    "        # check that all labels values are within the range of the number of classes\n",
    "        assert torch.all(labels < num_labels), \"A label value is out of the range of the number of classes.\"\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "        # Check if the mask is not empty\n",
    "        # print(labels)\n",
    "        if mask.any():\n",
    "            pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "            true_labels = labels[mask].detach().cpu().numpy()\n",
    "            accuracy = accuracy_score(pred_labels, true_labels)\n",
    "        else:\n",
    "            print(\"Warning: No valid labels found for accuracy calculation.\")\n",
    "            accuracy = 0\n",
    "        loss = outputs.loss\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(valid_dataloader):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "                mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "                pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "                true_labels = labels[mask].detach().cpu().numpy()\n",
    "                accuracy = accuracy_score(pred_labels, true_labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_accuracies.append(accuracy)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
    "         Train Loss: {sum(losses)/len(losses)}\\\n",
    "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
    "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"suim_segformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT IN TEST IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"BW\", 1: \"HD\", 2: \"PF\", 3: \"WR\", 4: \"RO\", 5: \"RI\", 6: \"FV\", 7: \"SR\"}\n",
    "palette = [[0, 0, 0], [0, 0, 255], [0, 255, 0], [0, 255, 255], [255, 0, 0], [255, 0, 255], [255, 255, 0], [255, 255, 255]]\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict in test images\n",
    "root_dir = 'SUIM'\n",
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "checkpoint = torch.load(\"suim_segformer.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def rescale_image(image, new_shape):\n",
    "    return cv2.resize(image, (new_shape[1], new_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def get_predictions(predictions_numpy, batch):\n",
    "    # Get the image sizes\n",
    "    original_shape = batch['pixel_values'].shape[-2:]\n",
    "    \n",
    "    # Rescale the predictions to the original image size\n",
    "    rescaled_predictions = [rescale_image(prediction, original_shape) for prediction in predictions_numpy]\n",
    "    \n",
    "    # Convert list of rescaled predictions to a NumPy array\n",
    "    rescaled_predictions = np.array(rescaled_predictions)\n",
    "    \n",
    "    # Get the original images and labels\n",
    "    images = batch['pixel_values'].cpu().numpy()\n",
    "    images = images.transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    return images, labels, rescaled_predictions\n",
    "\n",
    "# Plot the images, labels, and predictions\n",
    "def plot_predictions(images, labels, predictions):\n",
    "    fig, axs = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
    "    for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
    "        axs[i, 0].imshow(image)\n",
    "        axs[i, 0].set_title(\"Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(label)\n",
    "        axs[i, 1].set_title(\"Label\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        axs[i, 2].imshow(prediction)\n",
    "        axs[i, 2].set_title(\"Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def label_to_rgb(label, palette):\n",
    "    # Create an empty image with 3 channels for RGB\n",
    "    rgb_image = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Map each label to its corresponding color\n",
    "    for i, color in enumerate(palette):\n",
    "        mask = (label == i)\n",
    "        rgb_image[mask] = color\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def convert_predictions_and_labels_to_rgb(labels, predictions, palette):\n",
    "    labels_rgb = np.array([label_to_rgb(label, palette) for label in labels])\n",
    "    predictions_rgb = np.array([label_to_rgb(prediction, palette) for prediction in predictions])\n",
    "    return labels_rgb, predictions_rgb\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['pixel_values'].to(device)\n",
    "        filenames = batch['filename']\n",
    "        dims = batch['dim']\n",
    "\n",
    "        widths, heights = dims\n",
    "        widths_list = widths.tolist()\n",
    "        heights_list = heights.tolist()\n",
    "        dims_tuples = list(zip(widths_list, heights_list))\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Move predictions to CPU and convert to numpy for further processing if necessary\n",
    "        predictions_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "        # Process and plot predictions for the current batch\n",
    "        images, labels, rescaled_predictions = get_predictions(predictions_numpy, batch)\n",
    "\n",
    "        # Labels and predictions to RGB with palette\n",
    "        labels_rgb, predictions_rgb = convert_predictions_and_labels_to_rgb(labels, rescaled_predictions, palette)\n",
    "\n",
    "        for filename, prediction_rgb, dim in zip(filenames, predictions_rgb, dims_tuples):\n",
    "            all_predictions.append((filename, prediction_rgb, dim))\n",
    "\n",
    "# Reshape all predicitons to the original image size\n",
    "\n",
    "# Save the predictions as .png images\n",
    "output_dir = 'test_predictions_GT'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename, prediction, dim in all_predictions:  # Adjusted to unpack filenames with predictions\n",
    "    # Reshape prediction to the original image size\n",
    "    prediction = cv2.resize(prediction, dim, interpolation=cv2.INTER_NEAREST)\n",
    "    output_path = os.path.join(output_dir, filename)  # Use original filename\n",
    "    cv2.imwrite(output_path, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
