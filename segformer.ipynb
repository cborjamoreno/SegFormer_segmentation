{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, TFSegformerForSemanticSegmentation\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "# import albumentations as aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 640\n",
    "HEIGHT = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "\n",
    "        sub_path = \"train\" if self.train else \"test\"\n",
    "        self.img_dir = os.path.join(self.root_dir, sub_path, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, sub_path, \"masks\")\n",
    "\n",
    "        print(self.img_dir)\n",
    "        print(self.ann_dir)\n",
    "        \n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct paths to the image and annotation\n",
    "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        annotation_path = os.path.join(self.ann_dir, self.annotations[idx])\n",
    "\n",
    "        # Read the image and annotation using OpenCV\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        segmentation_map = cv2.imread(annotation_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Convert segmentation map IDs to your dataset's specific class IDs\n",
    "        num2id = {0: 0, 1: 29, 2: 150, 3: 179, 4: 76, 5: 105, 6: 226, 7: 255}\n",
    "        converted_map = np.zeros_like(segmentation_map)\n",
    "        for num, id_ in num2id.items():\n",
    "            converted_map[segmentation_map == id_] = num\n",
    "        segmentation_map = converted_map\n",
    "        \n",
    "\n",
    "        # Convert the OpenCV image to a PIL Image for the transformation\n",
    "        image = Image.fromarray(image)\n",
    "        segmentation_map = Image.fromarray(segmentation_map)\n",
    "\n",
    "        # Resize the image and segmentation map to a fixed size, e.g., 640x480\n",
    "        image = TF.resize(image, (HEIGHT, WIDTH))\n",
    "        segmentation_map = TF.resize(segmentation_map, (HEIGHT, WIDTH), interpolation=TF.InterpolationMode.NEAREST)\n",
    "\n",
    "        # Apply the transformations if any\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            # Convert the PIL Image to a tensor (this also permutes dimensions to C x H x W)\n",
    "            image = TF.to_tensor(image)\n",
    "\n",
    "        # Convert the PIL Image back to a NumPy array if your processing pipeline requires it\n",
    "        segmentation_map = np.array(segmentation_map)\n",
    "\n",
    "        # Convert the segmentation map to a tensor\n",
    "        segmentation_map = torch.tensor(segmentation_map, dtype=torch.long)\n",
    "\n",
    "\n",
    "        # Prepare the return dictionary\n",
    "        return_dict = {'pixel_values': image, 'labels': segmentation_map}\n",
    "\n",
    "        # Include the filename in the return dictionary if not in training mode\n",
    "        # Inside your method, after checking if not in training mode\n",
    "        if not self.train:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "            return_dict['filename'] = self.images[idx]\n",
    "            return_dict['dim'] = (width, height)\n",
    "\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(0.5),\n",
    "    # transforms.RandomVerticalFlip(0.05),\n",
    "    # transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUIM_sam/train/images\n",
      "SUIM_sam/train/masks\n",
      "SUIM_sam/test/images\n",
      "SUIM_sam/test/masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbm/miniconda3/envs/mmlab/lib/python3.7/site-packages/transformers/models/segformer/feature_extraction_segformer.py:31: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'SUIM_sam'\n",
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "test_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1508\n",
      "Number of validation examples: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 640])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = train_dataset[0]\n",
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 640])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [7, 7, 7,  ..., 0, 0, 0],\n",
       "        [7, 7, 7,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].squeeze().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = encoded_inputs[\"labels\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7a8e8004e6d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.8)  # 80% for training\n",
    "validation_size = total_size - train_size  # 20% for validation\n",
    "\n",
    "# Splitting the dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(validation_dataset, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 480, 640])\n",
      "labels torch.Size([4, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 480, 640])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"BW\", 1: \"HD\", 2: \"PF\", 3: \"WR\", 4: \"RO\", 5: \"RI\", 6: \"FV\", 7: \"SR\"}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"nvidia/mit-b0\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbm/miniconda3/envs/mmlab/lib/python3.7/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055feb870e3142d6a7810c6b334b5312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.5302797926070769         Train Loss: 1.0972559853697454         Val Pixel-wise accuracy: 0.6010728250585506         Val Loss: 0.7588376246000591\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7389f5a5078e45cd8f4c5fd06cc74d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.5242890194958882         Train Loss: 0.7569295957388468         Val Pixel-wise accuracy: 0.618010124946197         Val Loss: 0.7341100942147406\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a1a9bcb081400d9500522504ce1855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.5630905934725037         Train Loss: 0.6836442809231233         Val Pixel-wise accuracy: 0.5928414167962159         Val Loss: 0.680929549430546\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd19c9f004414d259c1b4a4ab3743f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.6123420927610438         Train Loss: 0.6288368023981322         Val Pixel-wise accuracy: 0.6499862632354408         Val Loss: 0.6849236641275255\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3562b823000e4224ae2ebc83a1c19c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.6316794680806311         Train Loss: 0.5963091291733925         Val Pixel-wise accuracy: 0.6796394060139272         Val Loss: 0.6747813812996212\n",
      "Epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7068dc2b49ad49b0ab7a2ee19e446124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.671269369017188         Train Loss: 0.563419638781358         Val Pixel-wise accuracy: 0.6264239670658829         Val Loss: 0.7286176112921614\n",
      "Epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c7a2e1e7d14dd996ebf430d9a3ca95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.6839182313535118         Train Loss: 0.5408895163741333         Val Pixel-wise accuracy: 0.5767354177800718         Val Loss: 0.7583509881637598\n",
      "Epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20f780347154a24b2fd6ebc9cd89fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7111808845625305         Train Loss: 0.5042291607288335         Val Pixel-wise accuracy: 0.6237991180247854         Val Loss: 0.7443433149080527\n",
      "Epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946c764bd1f4454f844d1a82e1ab18a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7186153988641287         Train Loss: 0.4904498422876099         Val Pixel-wise accuracy: 0.5096557649055103         Val Loss: 0.7147493207533109\n",
      "Epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ec9bcfe87540e8818e487520372355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7354722990134843         Train Loss: 0.45607084287515537         Val Pixel-wise accuracy: 0.4776033938661655         Val Loss: 0.8403007301845049\n",
      "Epoch: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8202f978477845338f62affe465441fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7576916245943677         Train Loss: 0.4316083509598347         Val Pixel-wise accuracy: 0.5539336889331885         Val Loss: 0.7653262830878559\n",
      "Epoch: 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efe5a2501ea4d59a532a89b513feba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7686379589415109         Train Loss: 0.41117927274167143         Val Pixel-wise accuracy: 0.5922959187319904         Val Loss: 0.7369572761419573\n",
      "Epoch: 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deff204d69f433ab738bc1e6a0e67ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7553213315909444         Train Loss: 0.42994164221531506         Val Pixel-wise accuracy: 0.5825940284107025         Val Loss: 0.8506498556388052\n",
      "Epoch: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6982c3a1184dbf9e36b0006c51fd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7642803435871195         Train Loss: 0.4091408687908918         Val Pixel-wise accuracy: 0.5976679058353183         Val Loss: 0.7982121831492374\n",
      "Epoch: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2739606a52d74b5da2574cd4398e3e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7865676976731008         Train Loss: 0.38891944323746575         Val Pixel-wise accuracy: 0.5430676046531273         Val Loss: 0.7890796653534237\n",
      "Epoch: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1699a93326af4e49a0829857545a10a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7922619897331231         Train Loss: 0.3754102958255256         Val Pixel-wise accuracy: 0.5864106714611181         Val Loss: 0.7778262586185807\n",
      "Epoch: 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ddaa5f683f4eaa8024edfde3e12996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7835142764742441         Train Loss: 0.38800684530411333         Val Pixel-wise accuracy: 0.6147783406139277         Val Loss: 0.8305719926169044\n",
      "Epoch: 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1c37340adc4fcfa592fbb3e7147d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7925648967678091         Train Loss: 0.37229857129963817         Val Pixel-wise accuracy: 0.6257057414578914         Val Loss: 0.8370725804645764\n",
      "Epoch: 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e782812f26475e8e58a2f38a6471df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.7983426408726019         Train Loss: 0.36155531540611724         Val Pixel-wise accuracy: 0.4818996220183146         Val Loss: 0.90935289271568\n",
      "Epoch: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63695282fd64d9aa08abffd62a38a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8023508194755339         Train Loss: 0.35833311006920227         Val Pixel-wise accuracy: 0.5499373890180592         Val Loss: 0.8566414690331409\n",
      "Epoch: 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c8d1987977400e9138df8da0c216d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8012601408457795         Train Loss: 0.3571754014551245         Val Pixel-wise accuracy: 0.5701284503184144         Val Loss: 0.8333668212749457\n",
      "Epoch: 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc9dbf8afd4813bcd5102585a52f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8122247549231701         Train Loss: 0.33961425588423055         Val Pixel-wise accuracy: 0.6370923196780638         Val Loss: 0.8461050322573436\n",
      "Epoch: 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f124deaf00a74366aa102c0c90ec6aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8102576764772772         Train Loss: 0.35091109196377906         Val Pixel-wise accuracy: 0.5661844404569819         Val Loss: 0.9139600931421706\n",
      "Epoch: 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7679db3c7b24c80be017f63d0bcf17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8034323528402835         Train Loss: 0.35397343756939403         Val Pixel-wise accuracy: 0.6189666624518483         Val Loss: 1.0458664280411445\n",
      "Epoch: 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3370b3a93ec543748fe682358b56f9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8172992984537414         Train Loss: 0.3430817266371076         Val Pixel-wise accuracy: 0.5532627799922336         Val Loss: 0.8925239561419738\n",
      "Epoch: 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4999f2bd0a4fe1a1a18b7538fbc3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8168265751953934         Train Loss: 0.3398746271796574         Val Pixel-wise accuracy: 0.5850025888606865         Val Loss: 1.0191925626836325\n",
      "Epoch: 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf55fdd39b4443090bc018af5931079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8300430788390847         Train Loss: 0.31903920386800705         Val Pixel-wise accuracy: 0.5957862738830955         Val Loss: 0.9325347358458921\n",
      "Epoch: 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c2ba0bbcc541b484f1972ba94785fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8268812700584808         Train Loss: 0.32237066303854744         Val Pixel-wise accuracy: 0.6245474463760297         Val Loss: 1.030823861689944\n",
      "Epoch: 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdca25606f7c4a3391ee6f9d8d101ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8164069837554428         Train Loss: 0.3221717900442367         Val Pixel-wise accuracy: 0.6119652564762942         Val Loss: 0.9085096269845963\n",
      "Epoch: 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f858924db74be1a5a346ec19e5435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pixel-wise accuracy: 0.8184402130519364         Train Loss: 0.324974844367891         Val Pixel-wise accuracy: 0.6013504205855146         Val Loss: 0.9979712518030092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(1, num_epochs+1):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # pixel_values = pixel_values.permute(0, 3, 1, 2)\n",
    "\n",
    "        # print(pixel_values.shape)\n",
    "        # print(pixel_values.dtype)\n",
    "        # print(labels.shape)\n",
    "        # print(labels.dtype)\n",
    "\n",
    "        # check that all labels values are within the range of the number of classes\n",
    "        assert torch.all(labels < num_labels), \"A label value is out of the range of the number of classes.\"\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "        # Check if the mask is not empty\n",
    "        # print(labels)\n",
    "        if mask.any():\n",
    "            pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "            true_labels = labels[mask].detach().cpu().numpy()\n",
    "            accuracy = accuracy_score(pred_labels, true_labels)\n",
    "        else:\n",
    "            print(\"Warning: No valid labels found for accuracy calculation.\")\n",
    "            accuracy = 0\n",
    "        loss = outputs.loss\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(valid_dataloader):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "                mask = (labels != 0) # we don't include the background class in the accuracy calculation\n",
    "                pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "                true_labels = labels[mask].detach().cpu().numpy()\n",
    "                accuracy = accuracy_score(pred_labels, true_labels)\n",
    "                val_loss = outputs.loss\n",
    "                val_accuracies.append(accuracy)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
    "         Train Loss: {sum(losses)/len(losses)}\\\n",
    "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
    "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"suim_segformer_sam.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT IN TEST IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"BW\", 1: \"HD\", 2: \"PF\", 3: \"WR\", 4: \"RO\", 5: \"RI\", 6: \"FV\", 7: \"SR\"}\n",
    "palette = [[0, 0, 0], [0, 0, 255], [0, 255, 0], [0, 255, 255], [255, 0, 0], [255, 0, 255], [255, 255, 0], [255, 255, 255]]\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbm/miniconda3/envs/mmlab/lib/python3.7/site-packages/transformers/models/segformer/feature_extraction_segformer.py:31: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Predict in test images\n",
    "root_dir = 'SUIM_sam'\n",
    "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    reshape_last_stage=True,\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "checkpoint = torch.load(\"suim_segformer_sam.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def rescale_image(image, new_shape):\n",
    "    return cv2.resize(image, (new_shape[1], new_shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def get_predictions(predictions_numpy, batch):\n",
    "    # Get the image sizes\n",
    "    original_shape = batch['pixel_values'].shape[-2:]\n",
    "    \n",
    "    # Rescale the predictions to the original image size\n",
    "    rescaled_predictions = [rescale_image(prediction, original_shape) for prediction in predictions_numpy]\n",
    "    \n",
    "    # Convert list of rescaled predictions to a NumPy array\n",
    "    rescaled_predictions = np.array(rescaled_predictions)\n",
    "    \n",
    "    # Get the original images and labels\n",
    "    images = batch['pixel_values'].cpu().numpy()\n",
    "    images = images.transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "    return images, labels, rescaled_predictions\n",
    "\n",
    "# Plot the images, labels, and predictions\n",
    "def plot_predictions(images, labels, predictions):\n",
    "    fig, axs = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
    "    for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
    "        axs[i, 0].imshow(image)\n",
    "        axs[i, 0].set_title(\"Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(label)\n",
    "        axs[i, 1].set_title(\"Label\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        axs[i, 2].imshow(prediction)\n",
    "        axs[i, 2].set_title(\"Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def label_to_rgb(label, palette):\n",
    "    # Create an empty image with 3 channels for RGB\n",
    "    rgb_image = np.zeros((label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Map each label to its corresponding color\n",
    "    for i, color in enumerate(palette):\n",
    "        mask = (label == i)\n",
    "        rgb_image[mask] = color\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def convert_predictions_and_labels_to_rgb(labels, predictions, palette):\n",
    "    labels_rgb = np.array([label_to_rgb(label, palette) for label in labels])\n",
    "    predictions_rgb = np.array([label_to_rgb(prediction, palette) for prediction in predictions])\n",
    "    return labels_rgb, predictions_rgb\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch['pixel_values'].to(device)\n",
    "        filenames = batch['filename']\n",
    "        dims = batch['dim']\n",
    "\n",
    "        widths, heights = dims\n",
    "        widths_list = widths.tolist()\n",
    "        heights_list = heights.tolist()\n",
    "        dims_tuples = list(zip(widths_list, heights_list))\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Move predictions to CPU and convert to numpy for further processing if necessary\n",
    "        predictions_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "        # Process and plot predictions for the current batch\n",
    "        images, labels, rescaled_predictions = get_predictions(predictions_numpy, batch)\n",
    "\n",
    "        # Labels and predictions to RGB with palette\n",
    "        labels_rgb, predictions_rgb = convert_predictions_and_labels_to_rgb(labels, rescaled_predictions, palette)\n",
    "\n",
    "        for filename, prediction_rgb, dim in zip(filenames, predictions_rgb, dims_tuples):\n",
    "            all_predictions.append((filename, prediction_rgb, dim))\n",
    "\n",
    "# Reshape all predicitons to the original image size\n",
    "\n",
    "output_dir = 'test_predictions_sam'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename, prediction, dim in all_predictions:  # Adjusted to unpack filenames with predictions\n",
    "    # Reshape prediction to the original image size\n",
    "    prediction = cv2.resize(prediction, dim, interpolation=cv2.INTER_NEAREST)\n",
    "    output_path = os.path.join(output_dir, os.path.splitext(filename)[0] + '.png')  # Use original filename\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(prediction, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
